apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-muti-process-with-onnx-gpu
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      app: test-muti-process
  template:
    metadata:
      labels:
        app: test-muti-process
      
    spec:
      containers:
      - name: onnxruntime-gpu
        image: mcr.microsoft.com/azureml/onnxruntime:latest-cuda
        volumeMounts:
        - mountPath: /my-test
          name: code-volume
        command: ["/bin/sh"]
        args:
          [
            "-c",
            "sleep inf",
          ]
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            tencent.com/vcuda-core: 25
            tencent.com/vcuda-memory: 10
          limits:
            tencent.com/vcuda-core: 25
            tencent.com/vcuda-memory: 10
      nodeSelector:
        kubernetes.io/hostname: dell02
      volumes:
      - name: code-volume
        hostPath:
          path: /OnnxSplitRunner
          type: DirectoryOrCreate
# apiVersion: v1
# kind: Pod
# metadata:
#   name: test-muti-process
#   annotations:
#     tencent.com/vcuda-core-limit: 25
# spec:
#   restartPolicy: Never
#   containers:
#   - image: mcr.microsoft.com/azureml/onnxruntime:latest-cuda
#     name: onnx-gpu-test
#     command:
#     - /usr/local/nvidia/bin/nvidia-smi
#     - pmon
#     - -d
#     - 10
#     resources:
#       requests:
#         tencent.com/vcuda-core: 25
#         tencent.com/vcuda-memory: 10
#       limits:
#         tencent.com/vcuda-core: 25
#         tencent.com/vcuda-memory: 10
#     vol